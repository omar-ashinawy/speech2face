# -*- coding: utf-8 -*-
"""Face_Decoder_integration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hiqm5wFbG1cBNGkoF4fWYHFg4lAbZyyv
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install keras_vggface
!pip install keras_applications
!pip install tensorflow_addons

!pip install keras_applications

import cv2
import dlib
import argparse
import time
import numpy as np
import matplotlib.pyplot as plt

import tensorflow_addons as tfa

from mlxtend.image import extract_face_landmarks

from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input

from keras_vggface.vggface import VGGFace

# Based on VGG16 architecture -> old paper(2015)
vggface = VGGFace(model='vgg16') # or VGGFace() as default

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras import layers
from keras.layers import Dense
from keras.layers import Conv2D
from keras.layers import Reshape
from keras.layers import Input, Lambda
from keras.layers import BatchNormalization
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from mlxtend.data import loadlocal_mnist

def image_warping(src_img, dest_landmarks):
    src_img = src_img.astype('uint8')
    dest_landmarks = dest_landmarks.astype('int32')
    dest_landmarks = dest_landmarks.reshape(68,2)
    src_img = src_img[0,:,:,:]
    #dest_landmarks = dest_landmarks[0,:]
    src_landmarks = extract_face_landmarks(src_img)
    # print(src_landmarks.dtype == dest_landmarks.dtype)
    src_landmarks[:,[0,1]] = src_landmarks[:,[1,0]]
    dest_landmarks[:,[0,1]] = dest_landmarks[:,[1,0]]
    expanded_src_landmarks = np.expand_dims(np.float32(src_landmarks), axis=0)
    expanded_dest_landmarks = np.expand_dims(np.float32(dest_landmarks), axis=0)
    expanded_src_img = np.expand_dims(np.float32(src_img), axis=0)

    warped_img, dense_flows = tfa.image.sparse_image_warp(expanded_src_img,
						  expanded_src_landmarks,
						  expanded_dest_landmarks,
						  interpolation_order=1,
						  regularization_weight=0.1,
						  num_boundary_points=4,
						  name='sparse_image_warp')
   
    return (warped_img[0]).numpy().astype(np.uint8)

class Landmark_Model():
    def __init__(self,model_save_path):
        self._vgg_model = VGGFace(model = 'vgg16')
        self.__save_callback = [tf.keras.callbacks.ModelCheckpoint(filepath = model_save_path)]
        self.model = None

    def __reset_model(self):
        self.model = None
  
    def load_model(self, model_path):
        self.__reset_model()
        self.model = keras.models.load_model(model_path)
  
    def summary(self):
        return self.model.summary()
  
    def train(self, x_train, y_train, batch_size, epochs, initial_epochs):
        self.model.fit(x = x_train, y = y_train, batch_size = batch_size, verbose = True, epochs = epochs, initial_epoch = initial_epochs, callbacks=self.__save_callback)
  
    def create_model(self):
        self.__reset_model()
        self.model = Sequential()
        for layer in self._vgg_model.layers[:-3]:
            self.model.add(layer)
            layer.trainable = False
        self._vgg_model = None
        self.model.add(Dense(1000, activation='relu', input_shape = (4096, ), name='Landmark_Model_input_layer'))
        
        self.model.add(BatchNormalization(name='Landmark_Model_Batch_1'))
        
        self.model.add(Dense(800, activation='relu',name='Landmark_Model_Dense_1'))
        self.model.add(Dense(600, activation='relu',name='Landmark_Model_Dense_2'))
        self.model.add(BatchNormalization(name='Landmark_Model_Batch_2'))

        self.model.add(Dense(400, activation='relu',name='Landmark_Model_Dense_3'))
        self.model.add(Dense(200, activation='relu',name='Landmark_Model_Dense_4'))
        self.model.add(BatchNormalization(name='Landmark_Model_Batch_3'))

        self.model.add(Dense(68*2, activation='relu',name='Landmark_Model_Dense_5'))
        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.5, decay = 0.000095)
        self.model.compile(optimizer=optimizer,loss='mse',metrics=[tf.keras.metrics.MeanSquaredError()])

class Texture_Model():
    def __init__(self, model_save_path):
        self.__vgg_model = VGGFace(model = 'vgg16')
        self.__save_callback = [tf.keras.callbacks.ModelCheckpoint(filepath = model_save_path)]
        self.model = None
    def __reset_model(self):
        self.model = None
    def create_model(self, complex_model):
        self.__reset_model()
        
        model = Sequential()

        for layer in self.__vgg_model.layers[:-3]:
            model.add(layer)
            layer.trainable = False
        self.__vgg_model = None

        model.add(Dense(4116, activation='relu', input_shape = (4096, ), name='Texture_Model_input_layer'))

        model.add(Dense(4116, activation='relu'))

        model.add(Dense(4116, activation='relu'))

        model.add(Reshape((7, 7, 84)))

        model.add(BatchNormalization())

        model.add(Conv2DTranspose(512, (3,3), strides=(2,2), padding='same'))

        model.add(Conv2D(512, (3,3), strides=(1,1), padding='same'))

        model.add(BatchNormalization())

        if complex_model:
            model.add(Conv2D(512, (3,3), strides=(1,1), padding='same'))
            model.add(BatchNormalization())

        model.add(Conv2DTranspose(512, (3,3), strides=(2,2), padding='same'))

        model.add(Conv2D(512, (3,3), strides=(1,1), padding='same'))

        model.add(BatchNormalization())

        if complex_model:
            model.add(Conv2D(512, (3,3), strides=(1,1), padding='same'))
            model.add(BatchNormalization())

        model.add(Conv2DTranspose(256, (3,3), strides=(2,2), padding='same'))

        model.add(Conv2D(256, (3,3), strides=(1,1), padding='same'))

        model.add(BatchNormalization())

        if complex_model:
            model.add(Conv2D(256, (3,3), strides=(1,1), padding='same'))
            model.add(BatchNormalization())

        model.add(Conv2DTranspose(128, (3,3), strides=(2,2), padding='same'))

        model.add(Conv2D(128, (3,3), strides=(1,1), padding='same'))

        model.add(BatchNormalization())

        model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same'))

        model.add(Conv2D(filters=3,kernel_size=(1,1),padding="same", activation="relu", name='prediction_layer'))

        # model.add(BatchNormalization())

        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.5, decay = 0.000095)

        model.compile(optimizer = optimizer, 
                    loss = 'mae',
                    metrics=[tf.keras.metrics.MeanAbsoluteError()])
        self.model = model
    def load_model(self, model_path):
        self.__reset_model()
        self.model = keras.models.load_model(model_path)
    def summary(self):
        return self.model.summary()
    def train(self, x_train, y_train, batch_size, epochs, initial_epochs):
        self.model.fit(x = x_train, y = y_train, batch_size = batch_size, verbose = True, epochs = epochs, initial_epoch = initial_epochs, callbacks=self.__save_callback)

class Face_Decoder():
    def __init__(self):
        
        input = keras.Input(shape=(4096,))
        
        self.texture_model = Sequential()
        texture = Texture_Model('./')
        texture.load_model('./drive/MyDrive/texture_Dataset/trained_models/Texture_model.h5')
        i = 0
        for layer in texture.model.layers:
            i = i + 1
            if i >= 23:
                layer.trainable = False
                layer._name = 'ensemble_' + str(i+1) + '_' + layer.name
                self.texture_model.add(layer)
        
        texture = None

        self.texture_model = self.texture_model(input)
        
        self.landmark_model = Sequential()
        landmark = Landmark_Model('./')
        landmark.load_model('/content/drive/MyDrive/model_landmark.h5')
        i = 0
        for layer in landmark.model.layers:
            i = i + 1
            if i >= 23:
                layer.trainable = False
                layer._name = 'ensemble_' + str(i+1) + '_' + layer.name
                self.landmark_model.add(layer)
        self.landmark_model = self.landmark_model(input)

        landmark = None

        self.model = Model(input, [self.texture_model, self.landmark_model])

    def predict(self,feature_vector):
        texture, landmark = self.model.predict(feature_vector)
        return image_warping(texture, landmark)

    def summary(self):
        return self.model.summary()
    
    def plot_model(self):
        tf.keras.utils.plot_model(self.model, show_shapes=True)

class Data_Loader():
    def __init__(self):
        pass
        
    @staticmethod
    def load_data(audios_path, faces_path, start_pos, n_samples):
        if not audios_path.endswith('/'):
            audios_path += "/"
        if not faces_path.endswith('/'):
            faces_path += "/"
        available_files = []
        files_counter = 0
        i = start_pos
        while files_counter < n_samples:
            current_audio = audios_path + "audio_" + str(i) + ".npy"
            current_face = faces_path + "face_" + str(i) + ".npy"
            if (not os.path.isfile(current_audio)) or (not os.path.isfile(current_face)): 
                i += 1
                continue
            available_files.append(i)
            i += 1
            files_counter += 1
        stop_pos = i
        # DO NOT REMOVE THIS PRINT STATEMENT
        print(available_files)
        audios_matrix = np.zeros((len(available_files), 598, 257, 2))
        faces_matrix = np.zeros((len(available_files), 4096)) 
        for i in range(len(available_files)):
            current_audio = audios_path + "audio_" + str(available_files[i]) + ".npy"
            current_face = faces_path + "face_" + str(available_files[i]) + ".npy"
            try:    
                with open(current_audio, 'rb') as f:
                    audios_matrix[i] = np.load(f)
                with open(current_face, 'rb') as f:
                    faces_matrix[i] = np.load(f).reshape((4096))
            except Exception as e:
                print(f'Couldn\'t Load Example: {str(available_files[i])}, Error:', e)
                continue
        return audios_matrix, faces_matrix, stop_pos

class Audio_Encoder():
    def __init__(self, model_save_path, alpha = 0.025, beta = 200, T = 2):
        self.model = self.__Audio_Encoder()
        self.__model_save_path = model_save_path
        self.__alpha = alpha
        self.__beta = beta
        self.__T = T
        inputs = tf.keras.Input(shape = (4096,))
        outputs = VGGFace(model = 'vgg16').layers[-1:][0](inputs)
        self.__loss_vgg_model = tf.keras.Model(inputs, outputs)

    def load_model(self, model_path):
        self.model.load_weights(model_path)

    def __Audio_Encoder(self):
        model = tf.keras.Sequential()

        model.add(tf.keras.layers.Input(shape = (598,257,2)))

        model.add(tf.keras.layers.Conv2D(64, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.Conv2D(64, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.Conv2D(128, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.MaxPool2D(pool_size=[2,1], strides=(2, 1)))

        model.add(tf.keras.layers.Conv2D(128, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.MaxPool2D(pool_size=[2,1], strides=(2, 1)))

        model.add(tf.keras.layers.Conv2D(128, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.MaxPool2D(pool_size=[2,1], strides=(2, 1)))

        model.add(tf.keras.layers.Conv2D(256, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.MaxPool2D(pool_size=[2,1], strides=(2, 1)))

        model.add(tf.keras.layers.Conv2D(512, (4, 4), strides=(1, 1), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='VALID'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())

        model.add(tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='VALID'))

        model.add(tf.keras.layers.AveragePooling2D(pool_size=(6,1), strides=1, padding="VALID"))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())
        
        model.add(tf.keras.layers.Flatten())
        
        model.add(tf.keras.layers.Dense(4096))
        model.add(tf.keras.layers.ReLU())
        
        model.add(tf.keras.layers.Dense(4096))	

        return model

    def paper_loss(self, y_true, y_pred):
        y_vgg_true = self.__loss_vgg_model(y_true)
        y_vgg_pred = self.__loss_vgg_model(y_pred)
        y_vgg_true_exp = tf.nn.softmax(y_vgg_true / self.__T)
        y_vgg_pred_exp = tf.nn.softmax(y_vgg_pred / self.__T)
        l_distill = -1 * tf.reduce_sum(tf.math.multiply(y_vgg_pred_exp, tf.math.log(y_vgg_true_exp)))
       
        y_true_norm = tf.norm(y_true)
        y_pred_norm = tf.norm(y_pred)
        mse = tf.reduce_sum(tf.math.reduce_euclidean_norm(y_true_norm - y_pred_norm))
        return self.__alpha*mse + self.__beta*l_distill

    def loss(self, y_true, y_pred):
        mae = tf.keras.losses.mean_absolute_error(y_true, y_pred)
        y_true /= tf.norm(y_true)
        y_pred /= tf.norm(y_pred)
        mse = tf.keras.losses.mean_squared_error(y_true, y_pred)
        return self.__alpha*mae + self.__beta*mse

    def plot_model(self, path):
        tf.keras.utils.plot_model(self.model, to_file = path, show_shapes=True)
    
    def train(self, audios_path, faces_path, use_prev_model, start_pos, n_samples, batch_size, start_epoch = 0, num_epochs = 7):
        saved_models = [f for f in os.listdir(self.__model_save_path) if os.path.isfile(os.path.join(self.__model_save_path, f))]
        if not(len(saved_models) == 0) and use_prev_model:
            last_saved_model = self.__model_save_path + saved_models[-1]
            self.model.load_weights(last_saved_model)
        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.5, decay = 0.000095)
        # don't forget to add parameters (function returns a function)
        self.model.compile(optimizer = optimizer, loss = self.paper_loss, metrics=['mean_absolute_error'])
        for epoch in range(start_epoch, start_epoch + num_epochs + 1):
            train_next_pos = start_pos
            loaded = 0
            while loaded < (n_samples//64):
                x_train, y_train, train_next_pos = Data_Loader.load_data(audios_path, faces_path, train_next_pos, 64)
                loaded += 1
                self.model.fit(x = x_train, y = y_train, validation_split = 0.125, batch_size = batch_size, verbose = True, epochs = epoch + 1, initial_epoch = epoch)
            if not self.__model_save_path.endswith('/'):
                self.__model_save_path += "/"
                if not os.path.exists(self.__model_save_path):
                    os.mkdir(self.__model_save_path)
            self.model.save_weights(self.__model_save_path + 'new_model_epoch_' + str(int(epoch)) + '.h5')
            print("Model saved after", epoch, "epoch")
            valid_next_pos = train_next_pos
            loaded = 0
            valid_loss = 0
            while loaded < (n_samples//64):         
                x_valid, y_valid, valid_next_pos = Data_Loader.load_data(audios_path, faces_path, valid_next_pos, 2)
                valid_loss += (len(x_valid)/batch_size)*self.model.evaluate(x_valid, y_valid, batch_size = batch_size, verbose=0)[0]
                loaded += 1
            print(f'Test Loss: {valid_loss}, after {epoch} epoch.')

class Audio_Preprocessor():
	def __init__(self, sr = 16000, frSize = 512, HopLength = 160, HannWindow = 400):
		# Preprocessing Parameters
		self.Sampling_Rate = sr
		self.Frame_Size = frSize
		self.Hop_Length = HopLength
		self.Hann_Window = HannWindow
		
	def Complex_Spectrogram(self, Audio_Path):

		# load Audio File
		input_data = tf.io.read_file(Audio_Path)
		audio, sr= tf.audio.decode_wav(input_data, desired_channels=1, desired_samples=self.Sampling_Rate)
		# check the Audio duration 
		if(audio.shape[0] < self.Sampling_Rate*6):
			audio = np.resize(audio, (self.Sampling_Rate*6,))

		elif(audio.shape[0] > self.Sampling_Rate*6):
			audio = audio[0:self.Sampling_Rate*6-1]

		# calculate the STFT
		Frequency_Components = tf.signal.stft(audio, fft_length = self.Frame_Size, frame_step = self.Hop_Length, frame_length = self.Hann_Window, window_fn = tf.signal.hann_window )
		# print(Frequency_Components.shape)

		# seperate the Real and imaginary parts and apply the Power low
		Real_Part      = np.real(Frequency_Components)
		Imaginary_Part = np.imag(Frequency_Components)

		Real_Part      = np.sign(Real_Part) * ( np.abs(Real_Part) ** 0.3 )
		Imaginary_Part = np.sign(Imaginary_Part) * ( np.abs(Imaginary_Part) ** 0.3 )

		return Real_Part, Imaginary_Part

class Speech_to_face():
	def __init__(self):
		self.audio_encoder = Audio_Encoder(True, './drive/MyDrive/models')
		self.audio_encoder.load_model('/content/drive/MyDrive/models/new_model_mse_beginning_24617_epoch_200.h5')
		self.__audio_Preprocessor = Audio_Preprocessor()
		self.face_decoder = Face_Decoder()

	def predict(self, audio_file_path):
		real_part, imaginary_part = self.__audio_Preprocessor.Complex_Spectrogram(audio_file_path)
		audio_features = 5 * np.array([real_part, imaginary_part]).reshape(((598, 257, 2)))
		audio_features = np.expand_dims(audio_features, axis = 0)
		audio_encoder_output = self.audio_encoder.model.predict(audio_features)
		face_decoder_output = self.face_decoder.predict(audio_encoder_output)
		return face_decoder_output, audio_encoder_output
	
	def plot_model(self):
		tf.keras.utils.plot_model(self.model, show_shapes=True)

speech2face = Speech_to_face()

current_index = '102383'
org_image = cv2.cvtColor(cv2.imread(f'./drive/MyDrive/cropped_photos/face_{current_index}.png'), cv2.COLOR_BGR2RGB)
vector = np.load(f'./drive/MyDrive/face_npy/face_{current_index}.npy').reshape(1, 4096)
image, audio_encoder_output = speech2face.predict(f'./drive/MyDrive/audio_files/{current_index}.wav')
# image, audio_encoder_output = speech2face.predict(f'/content/Deif.wav')
image = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2RGB)

print("Vectors Mean Absolute Error:", np.mean(np.abs(audio_encoder_output - vector).reshape(4096)))
plt.subplot(1, 2, 1)
plt.imshow(org_image)
plt.subplot(1, 2, 2)
plt.imshow(image)
plt.show()

import tensorflow_addons as tfa
import cv2
from mlxtend.image import extract_face_landmarks
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image 
import os
import PIL.Image
import scipy.ndimage

def align_image(src_file, face_landmarks, output_size=224, transform_size=4096, enable_padding=True, x_scale=1, y_scale=1, em_scale=0.1, alpha=False):


        lm = np.array(face_landmarks)
        lm_chin          = lm[0  : 17]  # left-right
        lm_eyebrow_left  = lm[17 : 22]  # left-right
        lm_eyebrow_right = lm[22 : 27]  # left-right
        lm_nose          = lm[27 : 31]  # top-down
        lm_nostrils      = lm[31 : 36]  # top-down
        lm_eye_left      = lm[36 : 42]  # left-clockwise
        lm_eye_right     = lm[42 : 48]  # left-clockwise
        lm_mouth_outer   = lm[48 : 60]  # left-clockwise
        lm_mouth_inner   = lm[60 : 68]  # left-clockwise

        # Calculate auxiliary vectors.
        eye_left     = np.mean(lm_eye_left, axis=0)
        eye_right    = np.mean(lm_eye_right, axis=0)
        eye_avg      = (eye_left + eye_right) * 0.5
        eye_to_eye   = eye_right - eye_left
        mouth_left   = lm_mouth_outer[0]
        mouth_right  = lm_mouth_outer[6]
        mouth_avg    = (mouth_left + mouth_right) * 0.5
        eye_to_mouth = mouth_avg - eye_avg

        # Choose oriented crop rectangle.
        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]
        x /= np.hypot(*x)
        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)
        x *= x_scale
        y = np.flipud(x) * [-y_scale, y_scale]
        c = eye_avg + eye_to_mouth * em_scale
        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])
        qsize = np.hypot(*x) * 2


        # Load in-the-wild image.
        if not os.path.isfile(src_file):
            print('\nCant find source image')
            return
        img = PIL.Image.open(src_file).convert('RGBA').convert('RGB')

        # Shrink.
        shrink = int(np.floor(qsize / output_size * 0.5))
        if shrink > 1:
            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))
            img = img.resize(rsize, PIL.Image.ANTIALIAS)
            quad /= shrink
            qsize /= shrink

        # Crop.
        border = max(int(np.rint(qsize * 0.1)), 3)
        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))
        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))
        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:
            img = img.crop(crop)
            quad -= crop[0:2]

        # Pad.
        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))
        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))
        if enable_padding and max(pad) > border - 4:
            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))
            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')
            h, w, _ = img.shape
            y, x, _ = np.ogrid[:h, :w, :1]
            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))
            blur = qsize * 0.02
            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)
            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)
            img = np.uint8(np.clip(np.rint(img), 0, 255))
            if alpha:
                mask = 1-np.clip(3.0 * mask, 0.0, 1.0)
                mask = np.uint8(np.clip(np.rint(mask*255), 0, 255))
                img = np.concatenate((img, mask), axis=2)
                img = PIL.Image.fromarray(img, 'RGBA')
            else:
                img = PIL.Image.fromarray(img, 'RGB')
            quad += pad[:2]

        # Transform.
        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)
        if output_size < transform_size:
            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)

        return np.asarray(img)

m = Sequential()
vgg = VGGFace(model = 'vgg16')
for layer in vgg.layers[:-3]:
    m.add(layer)
    layer.trainable = False

image = cv2.imread('/content/drive/MyDrive/Dataset/39048.png')
l = extract_face_landmarks(image)
image = align_image('/content/drive/MyDrive/Dataset/39048.png',l) 
l = extract_face_landmarks(image)
plt.imshow(image)
image = np.expand_dims(image, axis=0)
featureVector = m.predict(image)
print(featureVector.shape)

prediction = model.predict(featureVector)
print(prediction.shape)
prediction = cv2.cvtColor(prediction,cv2.COLOR_BGR2RGB)

plt.imshow(prediction)

!python -c 'import tensorflow as tf; print(tf.__version__)'

